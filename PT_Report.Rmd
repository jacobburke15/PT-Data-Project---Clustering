---
title: "MA679 Final Project - PT Data Analysis"
author: "Team 1"
date: "5/5/2020"
output:
  pdf_document: default
  html_document:
    df_print: paged
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning= FALSE, message=FALSE)
library(readxl)
library(ggplot2)
library(dplyr)
library(lubridate)
library(zoo)
library(scales)
library(tidyverse)
library(janitor)
library(gridExtra)
library(factoextra)
```


# Abstract   

Physical therapy has always been an intriguing topic on the data end, because a lot of features in this field would be hard to quantify. As statisticians, we always wonder the relationship between different variables. With the data from our client, we can now better understand what it would take to make a successful therapy and thus carry out further analysis.   

# Introduction 

For this project, our team decided on taking on the task of trying to determine what attributes may best differentiate multiple clusters of patients and how they vary within a provided PT patient data set. In theory, this would help our client better understand their overall patient audience, and potentially find niches they may have not be aware of before. 

To do this, we made a plan to perform unsupervised learning methods that included Autoencoders and K-Means Clustering to our provided data. Autoencoders are a common type of neural network used to encode high dimensional data into lower dimensions in an unsupervised fashion. Once our data is brought to a lower dimensions, we then planned to apply a K-means clustering layer. 

Once the data was clustered, we then could look to make interpretations and conclusions based on our original research question. Prior to EDA, we cleaned the provided data set and the information regarding our data cleaning process can be found in the Appendix. 
    


```{r, include = F}
dat2 <-  read_excel("For Masanao Class - ROMS Full Data Set - March 19th, 2019 Upload.xlsx",sheet = 1)
```

```{r, include = F}
# delete the weird ID 

dat2<- dat2 %>% filter(`ROMS ID` != 1000330)
# fix inconsistent admission date and discharge date.
# (it may merge some patients with relapse issues.)
dat2$`Visit Date` <- as.character(dat2$`Visit Date`)
dat2$`Admission Date`<- as.character(dat2$`Admission Date`)
dat2$`Discharge Date` <-as.character(dat2$`Discharge Date`)
# 1) fix the admission date
# for the (same discharge date + same patient ID + same classification + same body region + same outcome type)
dat2 <-dat2 %>% group_by(`ROMS ID`,Outcome,`Body Region`,Classification, `Discharge Date`) %>% mutate(`Admission Date` = ifelse(length(unique(`Admission Date`)) >=2, min(`Admission Date`),`Admission Date`))
# 2) fix the discharge date
# for the (same admission date + same patient ID + same classification + same body region + same outcome type)
dat2 <-dat2 %>% group_by(`ROMS ID`,Outcome,`Body Region`,Classification, `Admission Date`) %>% mutate(`Discharge Date` = ifelse(length(unique(`Discharge Date`)) >=2, max(`Discharge Date`),`Discharge Date`))
```

```{r, include = F}
# fix the age
dat2$Age <- floor(dat2$Age) 
# fix the typo for outcomes
dat2$Outcome[dat2$Outcome =="Neck DISABILITY INDEX" ] <-  "NECK DISABILITY INDEX"
dat2$Outcome[dat2$Outcome =="neck DISABILITY INDEX" ] <-  "NECK DISABILITY INDEX"
# fix the typo for `chronic pain`
dat2$`Chronic Pain (Yes/No)`[dat2$`Chronic Pain (Yes/No)` =="yes"] <- "Yes"
dat2$`Chronic Pain (Yes/No)`[dat2$`Chronic Pain (Yes/No)` =="no"] <- "No"
dat2$`Chronic Pain (Yes/No)`[dat2$`Chronic Pain (Yes/No)` ==1] <- "Unknown"
# fix the typo for body regions
dat2$`Body Region`[dat2$`Body Region` == "knee"] <- "Knee"
dat2$`Body Region`[dat2$`Body Region` == "lumbar"] <- "Lumbar"
# remove the duplicated rows and select some columns.
#dat2 <- dat2 %>% distinct() 
require(janitor)
dat2$`Injury Date` <- as.numeric(as.character(dat2$`Injury Date`))
dat2$`Injury Date` <- janitor::excel_numeric_to_date(dat2$`Injury Date`, date_system = "modern")
dat2$`Surgery Date` <- as.numeric(as.character(dat2$`Surgery Date`))
dat2$`Surgery Date` <- janitor::excel_numeric_to_date(dat2$`Surgery Date`, date_system = "modern")
```

```{r, include = F}
#### fix the inconsistency of beg and final score
# 1) after grouping, if for one therapy the admission pain score is not consistent, use the first visit pain score.
dat2 <- dat2 %>%group_by(`ROMS ID`,Outcome,`Body Region`, Classification,`Admission Date`,`Discharge Date`) %>% mutate(`Admission Pain` = ifelse( length(unique(`Admission Pain`))>=2, (first(`Visit Pain Score`) ), `Admission Pain` ))
# 2) after grouping, if for one therapy the admission outcome score is not consistent, use the first visit outcome score.
dat2<- dat2 %>%group_by(`ROMS ID`,Outcome,`Body Region`,  Classification,`Admission Date`,`Discharge Date`) %>% mutate(`Admission Outcome Score` = ifelse( length(unique(`Admission Outcome Score`))>=2, (first(`Visit Outcome Score`) ), `Admission Outcome Score` ))
# 3) after grouping, if for one therapy the discharge pain score is not consistent, use the last visit pain score.
dat2$`Discharge Pain Score` <- as.numeric(dat2$`Discharge Pain Score`)
dat2<- dat2 %>% group_by(`ROMS ID`,Outcome, `Body Region`, Classification,`Admission Date`,`Discharge Date`) %>% mutate(`Discharge Pain Score`= ifelse( length(unique(`Discharge Pain Score`))>=2, (last(`Visit Pain Score`) ), `Discharge Pain Score` ))

# 4) after grouping, if for one therapy the discharge outcome score is not consistent, use the last visit discharge outcome score.

dat2<- dat2 %>% group_by(`ROMS ID`,Outcome, `Body Region`, Classification,`Admission Date`,`Discharge Date`) %>% mutate(`Discharge Outcome Score`= ifelse(length(unique(`Discharge Outcome Score`))>=2, (last(`Visit Outcome Score`) ), `Discharge Outcome Score` ))

# fix the injury date, take the earliest.
# 1) in general
dat2$`Injury Date`<- as.character(dat2$`Injury Date`)
dat2 <- dat2 %>%group_by(`ROMS ID`,Outcome,`Body Region`, Classification,`Admission Date`,`Discharge Date`) %>% mutate(`Injury Date` = ifelse( length(unique(`Injury Date`))>=2, (min(`Injury Date`) ), `Injury Date` ))
# 2) fix ID 882's injury date
dat2 <- dat2 %>%group_by(`ROMS ID`,`Body Region`, Classification,`Admission Date`,`Discharge Date`)%>% 
 mutate(`Injury Date` = ifelse( length(unique(`Injury Date`))>=2, (min(`Injury Date`) ), `Injury Date` ))
```

```{r, include = F}
# evaluate the clinical function for the pain level. if the pain in the last decreases at least 2 or from less than 2 to 0, we say it's good therapy, otherwise, bad therapy.
da_evaluate <- dat2  %>% mutate(pain_effect = ifelse((`Discharge Pain Score`-`Admission Pain`) <= -2 & `Admission Pain` >= 2, "good",ifelse(`Admission Pain` < 2 &`Discharge Pain Score`==0, "good","bad" )))
# evaluate the treatment effectiveness overall.
# 1)  1. for the outcome type =LOWER EXTREMITY FUNC SCALE, it's good discharge outcome score if `improve >= 9` or from >71 to max80.
#     2. if either the admission or outcome discharge score greater than 80 (the standard scale for this type), remove.
da_evaluate1<- da_evaluate %>%  filter(Outcome == "LOWER EXTREMITY FUNC SCALE") %>% mutate(effect_all = ifelse(`Admission Outcome Score`<71 & (`Discharge Outcome Score`-`Admission Outcome Score`) >= 9, "good", ifelse(`Admission Outcome Score`>=71 & `Discharge Outcome Score`==80, "good", "bad"))) #%>% filter(`Admission Outcome Score`<=80 & da$`Discharge Outcome Score` <=80)
da_evaluate1 <- da_evaluate1 %>% filter(`Admission Outcome Score`<=80) %>% filter(`Discharge Outcome Score` <=80) 
# 2) deal with the outcome type =KNEE OUTCOME SURVEY, good if improve >= 9 or improve from >91 to 100, otherwise, bad.
da_evaluate2<- da_evaluate %>%  filter(Outcome == "KNEE OUTCOME SURVEY") %>% mutate(effect_all = ifelse(`Admission Outcome Score`<91 & (`Discharge Outcome Score`-`Admission Outcome Score`) >= 9, "good", ifelse(`Admission Outcome Score`>=91 & `Discharge Outcome Score`==100, "good", "bad"))) 
da_evaluate2 <- da_evaluate2 %>% filter(`Admission Outcome Score`<= 100) %>% filter(`Discharge Outcome Score` <=100) 
# 3)check the outcome type in "MODIFIED LOW BACK DISABILITY QUESTIONNAIRE","Quick DASH","NECK DISABILITY INDEX"
# (good if decreases more than 10, or decreases from less than 10 to 0, otherwise, bad.)
da_evaluate3 <- da_evaluate %>%  filter(Outcome %in% c("MODIFIED LOW BACK DISABILITY QUESTIONNAIRE","Quick DASH","NECK DISABILITY INDEX")) %>% mutate(effect_all = ifelse(`Admission Outcome Score`>10 & (`Discharge Outcome Score`-`Admission Outcome Score`) <= -10, "good", ifelse(`Admission Outcome Score` <=10 & `Discharge Outcome Score`==0, "good", "bad"))) 
da_evaluate3 <- da_evaluate3 %>% filter(`Admission Outcome Score`<= 100) %>% filter(`Discharge Outcome Score` <=100) 
# combine date
da_eval <- rbind(da_evaluate1,da_evaluate2,da_evaluate3)
# delete the distinct
da_eval  <- da_eval %>% distinct() 
```

```{r, include = F}
# we consider some features
subset <- da_eval[,c(2,5,6,7,8,15,16,17,18,20,21,22,25,26,27,28,29,30,31,32,33,37,38)]
# 4263 therapy?
subset.1 <- subset %>% group_by(`ROMS ID`,Outcome,`Body Region`,Classification, `Admission Date`,`Admission Outcome Score`,`Admission Pain`,`Discharge Date`,`Discharge Outcome Score`,`Discharge Pain Score`, pain_effect, effect_all) %>%  mutate(visit = n(), duration = ymd(`Discharge Date`)-ymd(`Admission Date`),age = Age, Gender = `Sex (1=male, 2=female)`,`Injury Date`,Surgical,`Chronic Pain (Yes/No)`,`Payer Category`)
# detected some conflict within age again.
# fix the age typo
subset.1$Age <- ifelse(subset.1$`ROMS ID` ==2435, 64, subset.1$Age)
subset.1$Age <- ifelse(subset.1$`ROMS ID` ==3539, NA, subset.1$Age)
subset.1$Age <- ifelse(subset.1$`ROMS ID` ==3957, 33, subset.1$Age)
# fix the conflict in chronic pain (by using unknown)
subset.1$`Chronic Pain (Yes/No)` <- ifelse(subset.1$`ROMS ID` %in% c(2435,2920,1418,2739), "Unknown", subset.1$`Chronic Pain (Yes/No)`)
subset.1$`Chronic Pain (Yes/No)` <- as.factor(subset.1$`Chronic Pain (Yes/No)`)
# gather features together in new dataframe
df <- subset.1 %>% select(`ROMS ID`,Age, Gender, Outcome,`Body Region`,Classification, `Admission Date`,`Admission Outcome Score`,`Admission Pain`,`Discharge Date`,`Discharge Outcome Score`,`Discharge Pain Score`, pain_effect, effect_all,visit, duration,`Injury Date`,Surgical,`Chronic Pain (Yes/No)`,`Payer Category`)
##ck <- df %>% group_by(`ROMS ID`,Age, Gender, Outcome,`Body Region`,Classification)%>% filter(n_distinct(`Payer Category`)>1)
df <- df %>% distinct()
# give score 
df <- df %>% mutate(painresult = ifelse(pain_effect =="good" , 1, 0)) %>% mutate(result = ifelse(effect_all =="good" , 1, 0))
#  2896 ID correspond to only 1 therapy (same period of time, same outcome type, same region, same classification)
subset.1 <- df %>% group_by(`ROMS ID`) %>% filter(n()==1)
# repeated situation - 1276 records correspond to multiple records, maybe different outcome, or different other criteria.
subset.2 <- df %>% group_by(`ROMS ID`) %>% filter(n()>1)
subset.2$painresult <- as.numeric(subset.2$painresult)
subset.2$result <- as.numeric(subset.2$result)
```

```{r, include = F}
# for the ID with repeated therapy summaries, check if the cause was outcome type
check <- subset.2 %>% group_by(`ROMS ID`,`Body Region`,Classification, `Admission Date`,`Discharge Date` )%>% mutate(rep = n_distinct(Outcome))
# 1) if the only cause is outcome type, we evaluate the therapy on average. 
check.1 <-check %>% filter(rep >=2 ) %>% mutate(painresult = ave(painresult),  result = ave(result),visit = ave(visit), duration = ave(duration))
check.1 <- check.1[, -c(23)]
#ck1<- check.1 %>% group_by(`ROMS ID`,Age, Gender, `Body Region`,Classification) %>% filter(n_distinct(`Injury Date`)>1)
# also when we evaluate them by their average, some information becomes not valid logically.
check.1[,c(4,8,9, 11,12,13,14)] <- NULL
check.1 <- distinct(check.1)
# 2) if the cause is not outcome type, the repeated therapy summary can imply that 2 therapy exist, whether the same time period or not. we shall keep those lines.
check.2 <- check %>% filter(rep ==1 ) %>% mutate(pain = ave(painresult),  effectiveness = ave(result),visit = ave(visit), duration = ave(duration))
check.2 <- check.2[, -c(23,24,25)]
```


```{r, include = F}
# combine the result, 3105 therapy in total
df_unique <- rbind(subset.1, check.2)
# combine the result including the repeated summaries of therapy, 3652 therapy in total
df_all <- dplyr::bind_rows(df_unique, check.1)
# remove the suspecious discharge date for id 2131, which discharge date is in June,2020
df_unique <- df_unique %>% filter(`ROMS ID` != 2131)
df_all <-df_all %>% filter(`ROMS ID` != 2131)
```

```{r, include = F}
length(df_unique$`ROMS ID`)
length(df_all$`ROMS ID`)
data <- df_unique
```

# EDA 

Once we had the data cleaned, we began EDA to gain a better understanding of the data that we were working with. We particularly wanted to look at the variance of variables within the PT patient data, as overall higher variance can compliment more decisive clustering. 

```{r, echo = F}
#da2$duration <- as.Date(da2$`Discharge Date`) - as.Date(da2$`Admission Date`)
data$`Admission Pain` <- as.factor(data$`Admission Pain`)
data <- data %>% drop_na(pain_effect)
ggplot(subset(data, `ROMS ID` != 2537 & 1893), aes(x=duration, color=`Admission Pain`)) + 
    geom_density()+
    geom_line(stat="density")+
    scale_colour_brewer(palette = "Spectral")+
    facet_wrap(pain_effect~.)+
    labs( x=("treatment duration (days)"), y = ("density"), title= ("Distribution of treatment duration given admission pain scores based on Outcome criteria"))+
    theme(plot.title = element_text(size =12))
```

From the plot on the left side, we can see that patients with higher admission pain score tend to have shorter durations which might be due to their change to surgery because of the bad outcome. On the other hand, for the good outcome score, patients with lower pain score would tend to stay for a certain amount of time because of the length of treatment. Patients with higher admission pain score would have longer treatment durations than others.    


```{r, echo = F}
# pain changes scores vs admission pain scores. green color means the outcome improvement reaches the outcome minimal clinical important difference, read color means the outcome improvement does not reach the outcome minimal clinical important difference.
# pain relief and whether the outcome reaches minimal clinical important difference are positively associated.
data$`Discharge Pain Score` <- as.numeric(data$`Discharge Pain Score`)
data$`Admission Pain` <- as.numeric(data$`Admission Pain`)
data2 <- data %>% mutate (change = `Discharge Pain Score` - `Admission Pain`)
data2$`Admission Pain`<-as.factor(data2$`Admission Pain`)
ggplot(data2, aes(x = `Admission Pain`, y = change))+ geom_boxplot(col = "gray", alpha = 0.3,fill= "gray15")+geom_jitter(height = 0.15, width = 0.25, aes(colour = factor(effect_all)),alpha = 0.7,size = 0.6)+labs(fill = ("Outcome Minimal Clinical Important Difference"), title=("Association between Pain changes and Whether reach outcome Minimal Clinical Important Difference"))+
    theme(plot.title = element_text(size =11))
```

From this plot, we can see the change in pain score between the admission and discharge of patients. The closer to the right of the graph, the more pain the patient had when he/she first came to the treatment. And the lower half of the graph represents a declining of pain score implay an improvement in the condition from the therapy. 


```{r, echo = F}
ggplot(dat2) +
  aes(x = Surgical, fill = `Chronic Pain (Yes/No)`) +
  geom_bar() +
  scale_fill_hue() +
  labs(title = "Surgical Vs. Non Surgical Injuries 
       by Chronic Pain")+
  theme_minimal()
```

From this plot, surgical injuries did not necessarily have more to do with the chronic pain. However, this plot could not show enough details that we need so we plotted the chronic pain and surgical condition versus body regions in the next part.    

```{r, echo = F}
ggplot(dat2) +
 aes(x = `Body Region`, fill = Surgical) +
 geom_bar() +
 scale_fill_hue() +
 labs(title = "Distribution of Body Region Injuries, 
      Surgical Vs. Non - Surgical") +
 theme_minimal() + theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Overall, we can see that not many of the patients would take the surgery since most of them are seeking physical therapies. However, we observed that the highest proportion for body injuries for which the patient would take a surgery is a knee pain. Besides knee, we also noticed that foot/ankle and shoulder injuries would be more likely to result in a surgery than injuries in other body regions.     



```{r, echo = F}
ggplot(dat2) +
  aes(x = `Body Region`, fill = `Chronic Pain (Yes/No)`) +
  geom_bar() +
  scale_fill_hue() +
  labs(title = "Distribution of Body Region Injuries, 
                  Chronic vs. Not") +
  theme_minimal()+ theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Injuries over most of body regions would result in a chronic pain more likely than taking a surgery. comparing to the former plot, the rise on the rate is obvious excpet for the body regions that were more likely to result in surgeries.    



```{r, echo = F}

data2 <- filter(dat2, `Body Region` == 'Balance' | `Body Region` == 'Cervical' |
                  `Body Region` =='Elbow' | `Body Region` == 'Foot/Ankle' | 
                  `Body Region` == 'Hand'| `Body Region` =='Hip')

data3 <- filter(dat2, `Body Region` == 'knee' | `Body Region` == 'Knee' |
                  `Body Region` =='lumbar' | `Body Region` == 'Lumbar' | 
                  `Body Region` == 'Pelvis'| `Body Region` == 'Shoulder' |
                  `Body Region` == 'Thoracic'| `Body Region` == 'Wrist' )

data3$`Body Region`[data3$`Body Region` == 'knee'] <- 'Knee'
data3$`Body Region`[data3$`Body Region` == 'lumbar'] <- 'Lumbar'


a <- ggplot(data2) +
  aes(x = log(Visits), color = `Body Region`) +
  geom_density(adjust = 1L, alpha = 0.5, size = 1) +
  scale_fill_hue() +
  theme_minimal()

b <- ggplot(data3) +
  aes(x = log(Visits), color = `Body Region`) +
  geom_density(adjust = 1L, alpha = 0.5, size = 1) +
  scale_fill_hue() +
  theme_minimal()

grid.arrange(a, b, ncol = 1, top = "Patient Visit Numbers by Injury Body Region")
```

We used logrithm on the number of visits to arrange them on the same scale and we can see that most of visit by injury body region are around 2 to 3. The exponential of 2.5 is around 12 so we suppose that was a treatment phase. Noticeably, for injury at pelvis, the log of visit is more smoothly distributed throughout the region rather than had a densed range like other body regions. This might be helpful for our later analysis.    

# Autoencoder & K-means Clustering 

Once EDA was complete, we had a clean 20-dimension data set that needed to be condensed before we then applied clustering. Therefore, we chose to use a variational autoencoder as a means of encoding the data to 2-dimensions in an unsupervised fashion. Once we get the data in 2-dimensions, we can then apply the k-means algorithm to the data and then work towards interpretation. (ie. Determining what variables from the original PT data set may be responsible to the seperation/break-outs of clusters in 2-dimensions.) 

It is important to note, that during the autoencoding process, we had troubles with maintaining the consistency of the autoencoder's neural network 2-dimensional output. Therefore, after significant tweaking and testing, we decided that the best thing to do to ensure reproducibility of the clustering was to save our optimal output from the autoencoder. 

```{r, echo = F}

library(keras)

K <- keras::backend()

```


```{r, echo = F}

## reading in clean data set 

data <- read.csv("dat2_all.csv")

## removing index and last two columns 

data <- data[,-c(1, 22, 23)]

## removing NA's 

data <- na.omit(data)

```

The specific of the code run for the autoencoder can be found in the corresponding .Rmd file to this .pdf report. We had our autoencoder hyperparameters set to: 

- batch size = 50
- epochs = 20 
- original dimension = 20
- intermediate dimension = 12
- latent dimension = 2
- epsilon = 1

The activation function used when encoded into lower dimensions was "relu". 

```{r, echo = F}

## settin the parameters for the autoencoder

batch_size <- 50L
original_dim <- 20L
latent_dim <- 2L
intermediate_dim <- 12L
epochs <- 20L
epsilon_std <- 1.0

```



```{r, echo = F}

## Defining the model

x <- layer_input(shape = c(original_dim))
h <- layer_dense(x, intermediate_dim, activation = "relu")
z_mean <- layer_dense(h, latent_dim)
z_log_var <- layer_dense(h, latent_dim)

sampling <- function(arg){
  z_mean <- arg[, 1:(latent_dim)]
  z_log_var <- arg[, (latent_dim + 1):(2 * latent_dim)]
  
  epsilon <- k_random_normal(
    shape = c(k_shape(z_mean)[[1]]), 
    mean=0.,
    stddev=epsilon_std
  )
  
  z_mean + k_exp(z_log_var/2)*epsilon
}

## z here is the latent data condensed from the x data 

z <- layer_concatenate(list(z_mean, z_log_var)) %>% 
  layer_lambda(sampling)


 decoder_h <- layer_dense(units = intermediate_dim, activation = "relu")
 decoder_mean <- layer_dense(units = original_dim, activation = "sigmoid")
 h_decoded <- decoder_h(z)
 x_decoded_mean <- decoder_mean(h_decoded)

# end-to-end autoencoder

vae <- keras_model(x, x_decoded_mean)

# encoder, from inputs to latent space

encoder <- keras_model(x, z_mean)

# generator, from latent space to reconstructed inputs (ie. decoder)

 decoder_input <- layer_input(shape = latent_dim)
 h_decoded_2 <- decoder_h(decoder_input)
 x_decoded_mean_2 <- decoder_mean(h_decoded_2)
 generator <- keras_model(decoder_input, x_decoded_mean_2)


## loss function 

vae_loss <- function(x, x_decoded_mean){
  xent_loss <- (original_dim/1.0)*loss_binary_crossentropy(x, x_decoded_mean)
  kl_loss <- -0.5*k_mean(1 + z_log_var - k_square(z_mean) - k_exp(z_log_var), axis = -1L)
  xent_loss + kl_loss
}

vae %>% compile(optimizer = "rmsprop", loss = vae_loss, experimental_run_tf_function = FALSE)

```


```{r, echo = F}

## preparing the data, splitting into testing and training 

## 75% of the sample size

smp_size <- floor(0.75 * nrow(data))

## set the seed to make the partition reproducible
set.seed(2020)

train_ind <- sample(seq_len(nrow(data)), size = smp_size)

x_train <- data[train_ind, ]
x_test <- data[-train_ind, ]

## getting training and testing into matrix form to be able to be passed into the fit() function 

x_train <-  matrix(unlist(x_train), ncol = 20, byrow = F)
x_test <- matrix(unlist(x_test), ncol = 20, byrow = F)

```



```{r, include = F}

## Training the model. 

## This is the end to end autoencoder. 

vae %>% fit(
  x_train, x_train, 
  shuffle = TRUE, 
  epochs = epochs, 
  batch_size = batch_size,
  validation_data = list(x_test, x_test), 
)

```

## Visualizing Encoded Data

```{r, include = F}

### Important to note, this section is not used when reading in the saved data from the 
### encoder in the chunk below

## Visualizing the encoded data

x_test_encoded <- predict(encoder, x_test, batch_size = batch_size)

## plotting the encoded 2-deminsions

x_test_encoded %>%
  as_data_frame() %>% 
  ggplot(aes(x = V1, y = V2)) + geom_point()

```

```{r, echo = F}

## for reproducibility, reading in the saved test encoded data to apply clustering 

x_test_encoded <- read.csv("x_test_encoded_relu.csv")

df <- read.csv("x_test_relu.csv")

x_test_encoded %>%
  as_data_frame() %>% 
  ggplot(aes(x = V1, y = V2)) + geom_point()

```

## K-Means Clustering

Once our data is encoded in two dimension as seen above, we then want to apply k-means clustering and visualize our results. The first thing we need to do is determine the optimal cluster count. 

```{r, echo = F}

## k-means clustering

set.seed(2020)

x_test_encoded <- as_data_frame(x_test_encoded)

x_test_encoded <- x_test_encoded[,2:3]

## elbow method to get optimal k clusters

kmean_withinss <- function(k) {
  cluster <- kmeans(x_test_encoded, k)
  return (cluster$tot.withinss)
}

# Set maximum cluster 

max_k <-20

# Run algorithm over a range of k 

wss <- sapply(2:max_k, kmean_withinss)

# Create a data frame to plot the graph

elbow <-data.frame(2:max_k, wss)

# Plotting

ggplot(elbow, aes(x = X2.max_k, y = wss)) +
  geom_point() + ggtitle("Elbow Graph to Find Optimal K-Means Cluster Number")+
  geom_line() +
  labs(x = "K Cluster Number", y = "WSS") +
  scale_x_continuous(breaks = seq(1, 20, by = 1))

```

From this graph we can see that the total within sum of squares for clusters drops significantly until about 8, and from 8 the drop of WSS begins to plateau. Therefore 8 clusters is an efficient cluster number, and we'll set that number for our clustering going forward. 

```{r, echo = F}

opt_clusters <- kmeans(x_test_encoded, 8)

## visualizing 

## print(opt_clusters)
str(opt_clusters)

```

From this output we can see that the clustering is showing a significantly lower value of total within sum of squares, compared to it's between sum of squares value. This is telling us that when running the k-means algorithm on our encoded data with 8 clusters, we are getting clusters that encapsulate points that are overall much closer to eachother, compared to their distance to points in _other_ clusters, and that is a good thing to see when running k-means. 

```{r, echo = F}

## visualizing the clustering

fviz_cluster(opt_clusters, data = x_test_encoded)

```

Now, here we can see that the clusters broke out fairly nicely. Clearly the largest divide of clusters is along the horizontal 'V1' axis. 

# Interpretations

Now that we have the encoded data clustered, we need to get creative and to try and interpret the results. We need to see if we can somewhat mirror the clustered data by visualizing the data with respect to variables from the original PT data set. If we are able to decipher a clear way in which the PT patient groups divide themselves among clusters with respect to variables from the data's original dimensions, then we would then be able to interpret the meaning of the new arbitrary 'V1' and 'V2' variables that were created from the nueral network autoencoder. 

Therefore, what we need to now do is visualize the current encoded data with respect to original variables in the PT patient data. 

```{r, echo = F}

## colouring by body region 

  ggplot(x_test_encoded) + 
  geom_point(aes(x = V1, y = V2, color = df$Body.Region))+ ggtitle("Colored by Body Region Injury")

```

However, here we can see that there seems to be a strong mix overall here with respect to Body Region, and there is not any trend we can see here that resembles the clusters from above. 

```{r}

## colouring by visit 

ggplot(x_test_encoded) + 
  geom_point(aes(x = V1, y = V2, color = df$visit)) + ggtitle("Colored by Visit Counts")

```

Again, here coloring the data by Visit counts taken by the patients we see an overall mix amongst the data, without any clear trend and no decisive groupings like that of the clustering. 

```{r}

## colouring by gender 

ggplot(x_test_encoded) + 
  geom_point(aes(x = V1, y = V2, color = as.factor(df$Gender))) +
  ggtitle("Coloured by Gender")

```

Finally we  see here that when splitting the encoded data by gender, there also is no clear trend similar to the clustering groups. 

For the sake of report length, the rest of the data grouping visualizations can be found in the appendix. 

# Conclusion

Once we had the data clustered, we needed to get creative to try and derive the representations of the new 2 dimensions for the encoded data, with respect to variables in the original PT data set. However, in our case the results of the k-means clustering didn’t seem to mirror any other dividing of the data, based on PT data variables including Body Region Injuries, Chronic Pain, Surgery, Gender, and so on.  

The result of our clustering algorithm may seem unexpected, but it is possible. According to the EDA plot “Distributions of treatment duration given admission pain scores”, the probability density function curves of treatment duration show similar patterns with each other, which means that Admission Pain Score is not a factor of significant impact on the overall patient clustering. In addition to Admission Pain Score, the EDA plot “Patient Visit Numbers by Injury Body Region” indicates that the distributions of visit times corresponding to each body region are similar, which means that body region may not be an important factor for patient clustering. The EDA plot “Distribution of Body Region Injuries, Chronic vs. Not” agrees the opinion that body regions may not be an important factor for patient clustering. The chronic pain symptom doesn’t show specific focus on particular body regions. Instead, the chronic pain keeps proportional to the total counts of treatments.

Therefore, concluding from our analysis, running our autoencoder did not decipher a clear way in which the PT patient groups divide themselves among clusters, when the dimensions are brought down to 2-Dimension. The output of the autoencoder is dependent on the data inputted, and no clear factors within the data that divide the current data.

# Limitations and Discussion

Aside from the unscaled variables in our data set, another potential data issue is that our data set contains both numeric information and text information. Text data needs to be preprocessed prior to building an autoencoder, becasue the Keras package is typically applied to numeric data, as well as the K-means algorithm. One possible solution for handling text data in our case is to create dummy variables and let them represent different text data. However, K-means algorithm performs poorly on categorical/dummy variable data. The reason for this is that the cost function of K-means algorithm usually computes the Euclidean distance between two numeric values. However, it is not possible to define such distance between categorical values.

In order to further optimize the clustering, we also need to confirm the applicability of different activating functions and tune hyperparameters in the future. A better set of hyperparameters and activiting functions may generate better clustering results.

\newpage 

# Appendix


## EDA 

```{r}
ggplot(dat2, mapping = aes(x = `Admission Pain`, fill = `Body Region`)) +
    geom_bar(position="stack")+coord_flip() + labs(title = ("Admission Pain and the Distribution of Body Region"))
```

```{r}
ggplot(dat2) +
  aes(x = `Length Of Stay (days)`, y = `Pain Change Scores`) +
  geom_point() + facet_wrap(~ `Body Region`) + theme_minimal()

ggplot(dat2) +
    aes(x =`Length Of Stay (days)`, y = `Outcome Change Scores`) +
    geom_point() + facet_wrap(~ `Body Region`)

  
ggplot(dat2) +
  aes(x = `Visits`, y = `Pain Change Scores`) +
  geom_point() + facet_wrap(~ `Body Region`)

ggplot(dat2) +
  aes(x =`Visits`, y = `Outcome Change Scores`) +
  geom_point() + facet_wrap(~ `Body Region`)
```


```{r}
ggplot(dat2) +
  aes(x = `Chronic Pain (Yes/No)`, y = `Outcome Change Scores`) +
  geom_boxplot(fill = "#0c4c8a") +
  labs(title = "Chronic Pain against Overall Pain Change Scores") +
  theme_minimal()

## doesn't seem to have much of an effect on Outcome score changes 

ggplot(dat2) +
  aes(x = `Chronic Pain (Yes/No)`, y = `Pain Change Scores`) +
  geom_boxplot(fill = "#0c4c8a") + 
  labs(title = "Chronic Pain against Overall Pain Change Scores") +
  theme_minimal()

```

## Interpretation Groupings 

```{r}
## colouring by admission pain 

ggplot(x_test_encoded) + 
  geom_point(aes(x = V1, y = V2, color = df$Admission.Pain)) + ggtitle("Colored by Admission Pain")

## colouring by surgical

ggplot(x_test_encoded) + 
  geom_point(aes(x = V1, y = V2, color = df$Surgical)) + 
  ggtitle("Coloured by Surgical")

## colouring by Chronic 

ggplot(x_test_encoded) + 
  geom_point(aes(x = V1, y = V2, color = df$Chronic.Pain..Yes.No.)) + 
  ggtitle("Colored by Chronic Pain")

```

## Explanation of Data Cleaning


Our data cleaning process is basically divided into 3 steps. Firstly, we looked into each column, detected and dealt with unusual values. Secondly, we determined the effectiveness of each therapeutic process based on the evaluation criteria of each outcome type. Specifically, we evaluated the effectiveness of pain reduction and the overall improvement. Then, we aggregated some therapy summaries which were logically duplicated, and reevaluated them by taking average. 

### i)

In the first step, we mainly worked on fixing typos, removing repeated rows, detected and fixed (or removed) problematic values. Typos are general problems caused by inconsistent upper/lower case spelling. And there are many special problems for each column, we analyzed and determined how to smooth the potential errors:

(a) We deleted the record with discharge date in June 2020. 

(b) We deleted a 7-digit ROMS ID.

(c) We deleted the negative age and adjusted the inconsistent age based on earlier therapy record.

(d) We adjusted chronic pain as NA if values for chronic pain conflicted with itself. 

(e) We fixed inconsistent admission date and discharge date. Particularly, holding other criteria constant (ID, classification, body region, outcome and admission date), if the discharge date varied, then we adjusted all the discharge dates to be the same as the last discharge date of this therapy. Similarly, holding other criteria constant (ID, classification, body region, outcome and discharge date), if the admission date varied, then we adjusted all the admission dates to be the same as the first admission date of this therapy.

(f) After the data cleaning steps mentioned above, we grouped the data by conditions including ROMS ID, Body Region, Outcome, Classification, Admission Date and Discharge Date. We defined each group of data as a combination of therapy records during one entire treatment process.  Then we recounted visit times and the time(days) during one entire therapy.

(g) We removed the outcome scores that were out of range. The maximum possible value for LOWER EXTREMITY FUNC SCALE is 80, we eliminated the records for those LOWER EXTREMITY FUNC SCALE values larger than 80 (applied to either admission scores or discharge scores).

### ii)

In the second step, we evaluated the effectiveness of pain reduction and the overall improvement of each treatment and gave binary scores to them.\newline

(a) For pain result, if pain scores were reduced by at least 2 points, or reduced from less than 2 to 0, then pain reduction effectiveness was good, we assigned 1 to the result. Otherwise, we considered the therapy’s effectiveness of pain reduction was not good, and we assigned 0 to the result.

(b) For overall outcome result, we needed to specify different criteria for different outcome types:
    (1) When outcome type was LOWER EXTREMITY FUNC SCALE, if the score increased at least 9 points or increased from larger than 71 to 80 (max), then we considered the treatment is effective overall, and we assigned the overall effect with score 1. Otherwise, we gave the overall effect with score 0.
    
    (2) when outcome type was KNEE OUTCOME SURVEY, if the score increased at least 9 points or increased from larger than 91 to 100 (max), then we considered the treatment is effective overall, and we assigned the overall effect with score 1. Otherwise, we gave the overall effect with score 0.
    
    (3) When outcome belongs to other 3 types, if the score decreased at least 10 points or decreased from less than 10 to 0 (minimum), then we considered the treatment is effective overall, and we assigned the overall effect with score 1. Otherwise, we gave the overall effect with score 0.

### iii)

After those 2 steps, then we aggregated some therapy summaries and evaluated them by taking average.	
For the treatments with identical same ID, body region, classification and treatment time, if the categories of outcome are different, we took the average of the pain reduction effectiveness scores (1 or 0) and took the average of the effect for overall improvement (1 or 0). Since we aggregated some rows, some columns became invalid logically, so they are assigned NA.