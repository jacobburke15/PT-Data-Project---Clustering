---
title: "Autoencoder - PT Data"
author: "Jacob Burke"
date: "20/04/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(ggplot2)
library(dplyr)
library(lubridate)
library(zoo)
library(scales)
library(tidyverse)
library(janitor)
library(standardize)


```

# Autoencoder using the PT Data 

Loading the keras package and back-end tensor flow. 

```{r, include = F}

library(keras)

K <- keras::backend()

```

Reading in Fiona's clean data "dat2_all.csv".

```{r}
data <- read.csv("dat2_all.csv")

## removing index and last two columns 

data <- data[,-c(1, 22, 23)]

## removing NA's 

data <- na.omit(data)


```

Setting the autoencoder parameters. 

```{r}

## batch size = 50, epochs = 20

batch_size <- 50L
original_dim <- 20L
latent_dim <- 2L
intermediate_dim <- 12L
epochs <- 20L
epsilon_std <- 1.0

```

Defining the model. 

```{r}

x <- layer_input(shape = c(original_dim))
h <- layer_dense(x, intermediate_dim, activation = "relu") ## not sure if this is the right activation
z_mean <- layer_dense(h, latent_dim)
z_log_var <- layer_dense(h, latent_dim)

sampling <- function(arg){
  z_mean <- arg[, 1:(latent_dim)]
  z_log_var <- arg[, (latent_dim + 1):(2 * latent_dim)]
  
  epsilon <- k_random_normal(
    shape = c(k_shape(z_mean)[[1]]), 
    mean=0.,
    stddev=epsilon_std
  )
  
  z_mean + k_exp(z_log_var/2)*epsilon
}

## z here is the latent data condensed from the x data 

z <- layer_concatenate(list(z_mean, z_log_var)) %>% 
  layer_lambda(sampling)

# we instantiate these layers separately so as to reuse them later 

## ***(may not need this decoder section once encoded for our PT data)***

decoder_h <- layer_dense(units = intermediate_dim, activation = "relu")
decoder_mean <- layer_dense(units = original_dim, activation = "sigmoid")
h_decoded <- decoder_h(z)
x_decoded_mean <- decoder_mean(h_decoded)

# end-to-end autoencoder
vae <- keras_model(x, x_decoded_mean)

# encoder, from inputs to latent space

encoder <- keras_model(x, z_mean)

# generator, from latent space to reconstructed inputs (ie. decoder)
decoder_input <- layer_input(shape = latent_dim)
h_decoded_2 <- decoder_h(decoder_input)
x_decoded_mean_2 <- decoder_mean(h_decoded_2)
generator <- keras_model(decoder_input, x_decoded_mean_2)


## loss function that is incorporated in Variational autoencoders 

vae_loss <- function(x, x_decoded_mean){
  xent_loss <- (original_dim/1.0)*loss_binary_crossentropy(x, x_decoded_mean)
  kl_loss <- -0.5*k_mean(1 + z_log_var - k_square(z_mean) - k_exp(z_log_var), axis = -1L)
  xent_loss + kl_loss
}

vae %>% compile(optimizer = "rmsprop", loss = vae_loss, experimental_run_tf_function = FALSE)

```

Preparing the data. 

```{r}

## need to split data into test and training 

## 75% of the sample size
smp_size <- floor(0.75 * nrow(data))

## set the seed to make your partition reproducible
set.seed(2020)

train_ind <- sample(seq_len(nrow(data)), size = smp_size)



x_train <- data[train_ind, ]
x_test <- data[-train_ind, ]

## getting them into matrix form to be able to be passed into the fit() function 

x_train <-  matrix(unlist(x_train), ncol = 20, byrow = F)
x_test <- matrix(unlist(x_test), ncol = 20, byrow = F)

```

Training the model. 

```{r}
## this is the end to end autoencoder 

vae %>% fit(
  x_train, x_train, 
  shuffle = TRUE, 
  epochs = epochs, 
  batch_size = batch_size,
  validation_data = list(x_test, x_test), 
)

```

Visualizing. 

```{r}
library(ggplot2)
library(dplyr)


x_test_encoded <- predict(encoder, x_test, batch_size = batch_size) ## passing in the encoder model with the test data

## plotting the encoded 2-deminsions

x_test_encoded %>%
  as_data_frame() %>% 
  ggplot(aes(x = V1, y = V2)) + geom_point()





```

Clustering on the Encoded data. 

```{r}

############ clustering attempt

set.seed(2020)

x_test_encoded <- as_data_frame(x_test_encoded)

clusters <- kmeans(x_test_encoded, 5)

result <- mutate(x_test_encoded, cluster = as.factor(clusters$cluster))

str(clusters)

## elbow method to get optimal k clusters

kmean_withinss <- function(k) {
  cluster <- kmeans(result, k)
  return (cluster$tot.withinss)
}

# Set maximum cluster 

max_k <-20

# Run algorithm over a range of k 

wss <- sapply(2:max_k, kmean_withinss)

# Create a data frame to plot the graph

elbow <-data.frame(2:max_k, wss)

# Plot the graph with gglop
ggplot(elbow, aes(x = X2.max_k, y = wss)) +
  geom_point() +
  geom_line() +
  labs(x = "K Cluster Number", y = "WSS") +
  scale_x_continuous(breaks = seq(1, 20, by = 1))

## just about after 6 clusters, the drop is fairly low dropping, so let's set to 8 for now 

opt_clusters <- kmeans(x_test_encoded, 8)


## visualizing attempt

print(opt_clusters)
str(opt_clusters)
library(factoextra)


fviz_cluster(opt_clusters, data = x_test_encoded)

## saved the test data set 

## write.csv(x_test, "x_test_relu.csv")

## saving the corresponding encoded test data set 

## write.csv(x_test_encoded, "x_test_encoded_relu.csv")


## two sets of output were saved and exported to github, one with the "elu" activation
## and the other with the "relu" activation 

## elu (50 batch, 20 epoch, 12 intermediate layer)

## relu (50 batch, 20 epoch, 12 intermediate layer)



```







